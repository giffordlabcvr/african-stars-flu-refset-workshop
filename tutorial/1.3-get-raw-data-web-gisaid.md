## 1.3 (Optional): Command-line approach with data downloaded from **GISAID** (web)

<img src="../images/gisaid.jpg" align="right" alt="" width="180"/>

### Download from GISAID (recommended filters)

1.  Open GISAID's **[EpiFlu](https://www.epicov.org/epi3/)** interface.

2.  Use the **Query Builder** (example):

    -   Virus/Subtype: **A/H3N2**
    -   Location: **South Africa**
    -   Collection date: **2018--2025**
    -   (Add any other constraints you need for your class/demo.)

3.  **Export sequences** as **FASTA**

    -   **Header template:** ensure the **first token is the GISAID ID** (`EPI_ISL_...`).
    -   It's fine to include extra fields after that, separated by `|`.
    -   Save as **`gisaid_ha.fasta`** in your working directory.

4.  **Export metadata** as **TSV/CSV**

    -   Choose **tab-delimited (TSV)**.
    -   Make sure the table includes at least: `Isolate_Id`, `Location`, `Collection_Date`, and (if available) `Clade`, `Host`, `Isolate_Name`.
    -   Save as **`gisaid_meta.tsv`**.

> Tip: Don't overthink column selection---our cleaning step later safely parses TSV and derives the fields Augur/Auspice need.

* * * * *

## Quick pre-flight checks (on your downloads)

Run these from your working directory (macOS/Linux):

### 1) Count sequences

`grep -c '^>' gisaid_ha.fasta`

### 2) Spot-check a few headers – the first token should be EPI_ISL_...

`grep '^>' gisaid_ha.fasta | head -5`

### 3) Confirm a '|' is present (not strictly required, but common)

`grep '^>' gisaid_ha.fasta | grep -c '|'`


## Next Steps

In the next steps we will:

1.  **Normalize** FASTA headers to the **left of the first `|`** and **de-duplicate** IDs;
2.  Run **Nextclade** (Docker) for QC + clades;
3.  Clean/merge **metadata**, then **downsample, align, and tree** with **Augur**;
4.  Export to **Auspice**;

Each step includes a brief explanation and a **checkpoint** so students can verify they're on track.

* * * * *

### 1.3.0. (One-time) Pull required images

```
docker pull nextstrain/nextclade:latest
docker pull nextstrain/base
```

* * * * *

### 1.3.1.  Normalize FASTA headers 

**a)** Normalize headers to the token before the first '|'.

Keep only the **ID** (left of the first `|`), then drop any duplicate IDs (keep first).

```
awk 'BEGIN{OFS=""}
  /^>/ { h=$0; sub(/^>/,"",h); split(h,a,"|"); id=a[1]; sub(/[[:space:]]+$/,"",id); print ">", id; next }
  { print }
' gisaid_ha.fasta > gisaid_ha.acc.fasta
```

**b)** De-duplicate by normalized ID

```
awk '/^>/{k=$0; keep=!seen[k]++} { if(keep) print }' \
  gisaid_ha.acc.fasta > gisaid_ha.acc.uniq.fasta
```

✅ **Checkpoint (should print nothing):**

```
grep '^>' gisaid_ha.acc.uniq.fasta | sort | uniq -d | sed -n '1,10p'
```

* * * * *

### 1.3.2. Nextclade QC + clades

<img src="../images/nextstrain.png" align="right" alt="" width="100"/> 

**What is Nextclade doing here?**

Nextclade compares each sequence to a **curated dataset** (reference, gene coordinates, clade rules, QC thresholds) and returns:

-   **Clade calls** (e.g., Nextstrain H3N2 clades),
-   **QC metrics** (overall pass/warn/fail + reasons),
-   Optional **aligned sequences** (if you ask for them).

We'll use the **H3N2 / HA** dataset so clade/QC logic matches your input.

**Why "dataset get"?**

This **downloads & caches** the pathogen-/gene-specific bundle under `~/.nextclade/...` so runs are fast and reproducible. We mount your host `~/.nextclade` into the container so the cache persists across runs.

**a)** Download the H3N2-HA dataset (cached under `~/.nextclade`)

```
docker run --rm -it -v "$HOME/.nextclade":/root/.nextclade nextstrain/nextclade:latest \
  nextclade dataset get --name nextstrain/flu/h3n2/ha \
  --output-dir /root/.nextclade/datasets/h3n2_ha
```

**b)** Run Nextclade on your normalized + deduped FASTA

```
docker run --rm -it \
  -v "$PWD":/data \
  -v "$HOME/.nextclade":/root/.nextclade \
  nextstrain/nextclade:latest \
  nextclade run \
    --input-dataset /root/.nextclade/datasets/h3n2_ha \
    --output-tsv /data/nextclade.tsv \
    /data/gisaid_ha.acc.uniq.fasta
```

✅ **Checkpoint**

-   `ls -lh nextclade.tsv` (file exists)
-   `wc -l nextclade.tsv` ≈ **#sequences + 1** (header)
-   Quick peek: `cut -f1,2,3,8 nextclade.tsv | head`\
    (you should see columns like `seqName`, `clade`, `qc.overallStatus`)

**What's in `nextclade.tsv` that we'll use later?**

-   `clade` → becomes `nextclade_clade` in our merged metadata.
-   `qc.overallStatus` → simple pass/warn/fail summary you can use to filter.
-   (There are many other QC fields; we keep it light for the tutorial.)

* * * * *

### 1.3.3. Clean GISAID metadata

<img src="../images/docker-python.jpg" align="right" alt="" width="100"/>

This trims to the fields Augur/Auspice need and derives `date/year/month` (safe TSV parsing; split `Location`; normalize dates).

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv, re
from datetime import datetime

inp  = "gisaid_meta.tsv"
outp = "meta_clean.tsv"

def split_loc(s):
    parts = [p.strip() for p in (s or "").split('/') if p.strip()]
    parts += [""] * (4 - len(parts))
    return parts[:4]  # region, country, division, location

def norm_date(s):
    if not s: return ("","","")
    s = s.strip()
    for fmt in ("%Y-%m-%d","%Y-%m","%Y"):
        try:
            dt = datetime.strptime(s, fmt)
            y = dt.year; m = dt.month if "%m" in fmt else 1
            return (dt.strftime("%Y-%m-%d") if "%d" in fmt else f"{y:04d}-{m:02d}-01",
                    str(y), f"{m:02d}")
        except ValueError: pass
    m = re.search(r"(\d{4})", s)
    if m: return (f"{m.group(1)}-01-01", m.group(1), "01")
    return ("","","")

with open(inp, newline='') as f, open(outp, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    cols_out = ["Isolate_Id","Isolate_Name","Subtype","Clade","Host",
                "region","country","division","location","Collection_Date",
                "date","year","month"]
    w = csv.DictWriter(g, delimiter="\t", fieldnames=cols_out); w.writeheader()
    for row in r:
        reg, ctry, div, loc = split_loc(row.get("Location",""))
        d, y, m = norm_date(row.get("Collection_Date",""))
        w.writerow({
          "Isolate_Id": row.get("Isolate_Id",""),
          "Isolate_Name": row.get("Isolate_Name",""),
          "Subtype": row.get("Subtype",""),
          "Clade": row.get("Clade",""),
          "Host": row.get("Host",""),
          "region": reg, "country": ctry, "division": div, "location": loc,
          "Collection_Date": row.get("Collection_Date",""),
          "date": d, "year": y, "month": m
        })
print(f"Wrote {outp}")
PY
```

✅ **Checkpoint:** `meta_clean.tsv` exists.\
(Optional sanity check counts:)

```
echo "FASTA seqs:" $(grep -c '^>' gisaid_ha.acc.uniq.fasta)
echo "metadata rows:" $(tail -n +2 meta_clean.tsv | wc -l)
```

* * * * *


### 1.3.4 Merge metadata + Nextclade

We map `nextclade.tsv` to `Isolate_Id` (the **same IDs** you put in FASTA headers).

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv

meta_in  = "meta_clean.tsv"
clade_in = "nextclade.tsv"
outp     = "metadata_final.tsv"
missed_log = "nextclade_unmatched_ids.txt"

clade_map = {}
with open(clade_in, newline='') as f:
    r = csv.DictReader(f, delimiter="\t")
    for row in r:
        acc = row['seqName'].split('|')[0]  # safe even if no '|'
        clade_map[acc] = {
          'nextclade_clade': row.get('clade',''),
          'qc.overallStatus': row.get('qc.overallStatus','')
        }

missed = set()
with open(meta_in, newline='') as f, open(outp, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    cols_out = ["strain","Isolate_Id","Isolate_Name","Subtype","Clade","nextclade_clade",
                "Host","region","country","division","location","Collection_Date",
                "date","year","month","qc.overallStatus"]
    w = csv.DictWriter(g, delimiter="\t", fieldnames=cols_out); w.writeheader()
    for row in r:
        iso = row["Isolate_Id"]
        nc  = clade_map.get(iso)
        if not nc: missed.add(iso); nc = {'nextclade_clade':'','qc.overallStatus':''}
        w.writerow({
          "strain": iso, "Isolate_Id": iso,
          "Isolate_Name": row["Isolate_Name"], "Subtype": row["Subtype"],
          "Clade": row["Clade"], "nextclade_clade": nc['nextclade_clade'],
          "Host": row["Host"], "region": row["region"], "country": row["country"],
          "division": row["division"], "location": row["location"],
          "Collection_Date": row["Collection_Date"], "date": row["date"],
          "year": row["year"], "month": row["month"],
          "qc.overallStatus": nc["qc.overallStatus"]
        })

with open(missed_log,"w") as h:
    for m in sorted(missed): h.write(m+"\n")

print(f"Wrote {outp}. Unmatched in Nextclade: {len(missed)} (see {missed_log})")
PY
```

✅ **Checkpoint:** `metadata_final.tsv` exists.\
(optional) `wc -l nextclade_unmatched_ids.txt` should be **0** (or small).

* * * * *

### 1.3.5 Augur: downsample, align, tree, refine

We'll keep **≤10 sequences per clade per year** (tune as needed

**a) Index** 

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur index \
    --sequences gisaid_ha.acc.uniq.fasta \
    --output raw.idx
```

**b) Filter**  (downsample):

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur filter \
    --metadata metadata_final.tsv \
    --sequences gisaid_ha.acc.uniq.fasta \
    --sequence-index raw.idx \
    --metadata-id-columns strain \
    --group-by nextclade_clade year \
    --sequences-per-group 10 \
    --output-sequences curated.fasta \
    --output-metadata curated.tsv
```

**c) Normalise South African province names (recommended for maps)**

Auspice expects canonical province names. This step cleans common variants (e.g., "Guateng", "Province of ...", different casings) so the **division** map layer renders correctly.

Run this on the downsampled metadata (`curated.tsv`) to produce `curated_geo.tsv`:

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv, re

infile  = "curated.tsv"      # or "metadata_final.tsv" if fixing earlier
outfile = "curated_geo.tsv"

canon = {
  "eastern cape":"Eastern Cape",
  "free state":"Free State",
  "gauteng":"Gauteng",
  "guateng":"Gauteng",
  "guanteng":"Gauteng",
  "?gauteng":"Gauteng",
  "gauteng province":"Gauteng",
  "kwazulu natal":"KwaZulu-Natal",
  "kwazulu-natal":"KwaZulu-Natal",
  "kwa-zulu natal":"KwaZulu-Natal",
  "province of kwazulu-natal":"KwaZulu-Natal",
  "kzn":"KwaZulu-Natal",
  "limpopo":"Limpopo",
  "mpumalanga":"Mpumalanga",
  "mpumalanga province":"Mpumalanga",
  "north west":"North West",
  "province of north-west":"North West",
  "northern cape":"Northern Cape",
  "western cape":"Western Cape",
  "province of the western cape":"Western Cape",
  "province of eastern cape":"Eastern Cape",
  "western cape ":"Western Cape",
  "western  cape":"Western Cape"
}

def norm_div(s):
  if not s: return s
  t = s.strip()
  t = re.sub(r'^(province of\s+)', '', t, flags=re.I)  # drop "Province of" prefix
  t = re.sub(r'[^\w\s\-]', '', t)                      # remove stray punctuation like '?'
  k = t.lower().strip()
  return canon.get(k, t)  # fall back to cleaned original if unknown

with open(infile, newline='') as f, open(outfile, "w", newline='') as g:
  r = csv.DictReader(f, delimiter="\t")
  w = csv.DictWriter(g, delimiter="\t", fieldnames=r.fieldnames)
  w.writeheader()
  for row in r:
    row["division"] = norm_div(row.get("division",""))
    # we keep 'location' untouched; we'll drop city-level mapping in export unless lat/longs are supplied
    loc = row.get("location","") or ""
    row["location"] = loc.strip()
    w.writerow(row)

print(f"Wrote {outfile}")
PY
```

✅ **Checkpoint:** `curated_geo.tsv` exists.

To eyeball:
```
awk -F'\t' 'NR==1{for(i=1;i<=NF;i++) if($i=="division"){c=i;break;}; next} {print $c}' \
  curated_geo.tsv | sort -u | sed -n '1,15p'
```
  
**d) Align** (or use Nextclade-aligned if you prefer):

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur align \
    --sequences curated.fasta \
    --output aligned.fasta \
    --fill-gaps
```

**e) Reconstruct tree**

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur tree \
    --alignment aligned.fasta \
    --output tree_raw.nwk \
    --nthreads 2
```

**f) Refine** (time-scale + metadata): 

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur refine \
    --tree tree_raw.nwk \
    --alignment aligned.fasta \
    --metadata curated_geo.tsv \
    --output-tree tree.nwk \
    --timetree
```

✅ **Checkpoint:** `curated.fasta`, `aligned.fasta`, `tree.nwk` exist.

```
echo "Curated seqs:" $(grep -c '^>' curated.fasta)
```


* * * * *


### 1.3.6 Export to Auspice & view

Make a directory for the auspice data.

```
mkdir -p auspice
```

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur export v2 \
    --tree tree.nwk \
    --metadata curated_geo.tsv \
    --color-by-metadata nextclade_clade Clade year month country division Host \
    --metadata-columns date \
    --geo-resolutions country division \
    --title "H3N2 South Africa (2018–2025)" \
    --maintainers "Workshop Team <team@example.org>" \
    --output auspice/gisaid-h3n2-ha.json
```

```
docker run -it --rm \
  -v "$PWD":/data -w /data \
  -p 4010:4000 \
  nextstrain/base \
  nextstrain view --host 0.0.0.0 auspice/
```

Open: `http://localhost:4010/gisaid-h3n2-ha`

✅ **Checkpoint:** In Auspice, try **Color by**: `nextclade_clade`, `year`, `country`.


* * * * *



### 1.3.7 (Optional) Add **your own sequences** to the tree

**Prepare your files**

-   `myseqs.fasta` (headers like `>MYSEQ_001`, `>MYSEQ_002`, ...)

-   `my_meta.tsv` (same columns as `metadata_final.tsv`; **must** have `strain` = FASTA header)

Minimal template (copy to `my_meta.tsv` and edit):

```
strain	Isolate_Id	Isolate_Name	Subtype	Clade	nextclade_clade	Host	region	country	division	location	Collection_Date	date	year	month	qc.overallStatus
MYSEQ_001	MYSEQ_001	A/South_Africa/Seq1/2023	H3N2		Human	Africa	South Africa	Western Cape	Cape Town	2023-05-15	2023-05-15	2023	05	pass
MYSEQ_002	MYSEQ_002	A/South_Africa/Seq2/2024	H3N2		Human	Africa	South Africa	Gauteng	Johannesburg	2024-02-01	2024-02-01	2024	02	pass
```

> Better: run Nextclade on `myseqs.fasta` too and fill `nextclade_clade`/`qc.overallStatus` from its TSV, but it's optional for the demo.

**Combine with GISAID**

Combine FASTA

```
cat gisaid_ha.acc.uniq.fasta myseqs.fasta > combined.fasta
```

Combine metadata (header from GISAID metadata_final.tsv)

```
(head -n 1 metadata_final.tsv && tail -n +2 metadata_final.tsv && tail -n +2 my_meta.tsv) \
  > combined_meta.tsv
```

**Re-run Augur on the combined set**

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur index --sequences combined.fasta --output raw.idx
```

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur filter \
    --metadata combined_meta.tsv \
    --sequences combined.fasta \
    --sequence-index raw.idx \
    --metadata-id-columns strain \
    --group-by nextclade_clade year \
    --sequences-per-group 10 \
    --output-sequences curated.fasta \
    --output-metadata curated.tsv

# (align, tree, refine, export, view) — same as Steps 5–6
```

✅ **Checkpoint:** your `MYSEQ_*` tips appear in Auspice alongside GISAID references.


* * * * *

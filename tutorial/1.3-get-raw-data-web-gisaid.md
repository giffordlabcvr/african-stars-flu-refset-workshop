## 1.3 (Optional): Command-line approach with data downloaded from **GISAID** (web)

To download the FASTA from GISAID:

-   Use the query builder: A/H3N2, Location = South Africa, Date = 2018--2025.
-   Export: **FASTA + metadata** (CSV/TSV).
-   Consider **[data-use terms](https://gisaid.org/terms-of-use/)**.

This tutorial will assume you've already downloaded **FASTA** + **metadata TSV** from the GISAID web interface after applying the appropriate filters (subtype, country, date range).

Data previously downloaded from GISAID is available **[here]()**, including `FASTA` formatted **[sequence data]()** and `.tsv` formatted **[sequence metadata]()**.

In this section we will:

1.  Start with a FASTA + metadata download from the GISAID web interface.
2.  Run **Nextclade** to get QC and clade calls.
3.  Clean metadata into a format Augur/Auspice understands.
4.  Downsample, align, build, and refine a tree with **Augur**.
5.  Export to **Auspice** for visualization.
6.  (Optional) Add **your own sequences** to see them alongside the GISAID references.


### Inputs

From GISAID (after filtering in their web interface):

-   `gisaid_ha.fasta`\
    *FASTA headers like:*

```
>EPI_ISL_19177765|A/South_Africa/R07785/2023
```

`gisaid_meta.tsv`\
*Metadata; tab-delimited with columns like `Isolate_Id`, `Location`, `Collection_Date`, etc.*
    
* * * * *

### 1.3.0. (One-time) Pull required images

```
docker pull nextstrain/nextclade:latest
docker pull nextstrain/base
```

* * * * *



### 1.3.1.  Normalize FASTA headers 

Keep only the **ID** (left of the first `|`), then drop any duplicate IDs (keep first).


```
# Normalize headers to the token before the first '|'
awk 'BEGIN{OFS=""}
  /^>/ { h=$0; sub(/^>/,"",h); split(h,a,"|"); id=a[1]; sub(/[[:space:]]+$/,"",id); print ">", id; next }
  { print }
' gisaid_ha.fasta > gisaid_ha.acc.fasta

# De-duplicate by normalized ID
awk '/^>/{k=$0; keep=!seen[k]++} { if(keep) print }' \
  gisaid_ha.acc.fasta > gisaid_ha.acc.uniq.fasta
```

✅ **Checkpoint (should print nothing):**

```
grep '^>' gisaid_ha.acc.uniq.fasta | sort | uniq -d | sed -n '1,10p'
```

* * * * *

### 1.3.2. Nextclade QC + clades

Get the H3N2-HA dataset (cached under `~/.nextclade`)

```
docker run --rm -it -v "$HOME/.nextclade":/root/.nextclade nextstrain/nextclade:latest \
  nextclade dataset get --name nextstrain/flu/h3n2/ha \
  --output-dir /root/.nextclade/datasets/h3n2_ha
```

2b. Run Nextclade

```
docker run --rm -it \
  -v "$PWD":/data \
  -v "$HOME/.nextclade":/root/.nextclade \
  nextstrain/nextclade:latest \
  nextclade run \
    --input-dataset /root/.nextclade/datasets/h3n2_ha \
    --output-tsv /data/nextclade.tsv \
    /data/gisaid_ha.acc.uniq.fasta
```

✅ **Checkpoint:** `nextclade.tsv` exists (try `head nextclade.tsv`).


### 1.3.3. Clean GISAID metadata

This trims to the fields Augur/Auspice need and derives `date/year/month` (safe TSV parsing; split `Location`; normalize dates).

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv, re
from datetime import datetime

inp  = "gisaid_meta.tsv"
outp = "meta_clean.tsv"

def split_loc(s):
    parts = [p.strip() for p in (s or "").split('/') if p.strip()]
    parts += [""] * (4 - len(parts))
    return parts[:4]  # region, country, division, location

def norm_date(s):
    if not s: return ("","","")
    s = s.strip()
    for fmt in ("%Y-%m-%d","%Y-%m","%Y"):
        try:
            dt = datetime.strptime(s, fmt)
            y = dt.year; m = dt.month if "%m" in fmt else 1
            return (dt.strftime("%Y-%m-%d") if "%d" in fmt else f"{y:04d}-{m:02d}-01",
                    str(y), f"{m:02d}")
        except ValueError: pass
    m = re.search(r"(\d{4})", s)
    if m: return (f"{m.group(1)}-01-01", m.group(1), "01")
    return ("","","")

with open(inp, newline='') as f, open(outp, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    cols_out = ["Isolate_Id","Isolate_Name","Subtype","Clade","Host",
                "region","country","division","location","Collection_Date",
                "date","year","month"]
    w = csv.DictWriter(g, delimiter="\t", fieldnames=cols_out); w.writeheader()
    for row in r:
        reg, ctry, div, loc = split_loc(row.get("Location",""))
        d, y, m = norm_date(row.get("Collection_Date",""))
        w.writerow({
          "Isolate_Id": row.get("Isolate_Id",""),
          "Isolate_Name": row.get("Isolate_Name",""),
          "Subtype": row.get("Subtype",""),
          "Clade": row.get("Clade",""),
          "Host": row.get("Host",""),
          "region": reg, "country": ctry, "division": div, "location": loc,
          "Collection_Date": row.get("Collection_Date",""),
          "date": d, "year": y, "month": m
        })
print(f"Wrote {outp}")
PY
```







GISAID TSVs have lots of columns. We'll trim down to the ones Augur needs and also split `Location` into `region / country / division / location`, plus normalize the collection date.

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv, re
from datetime import datetime

inp  = "gisaid_meta.tsv"
outp = "meta_clean.tsv"

def split_loc(s):
    parts = [p.strip() for p in (s or "").split('/') if p.strip()]
    parts += [""] * (4 - len(parts))
    return parts[:4]

def norm_date(s):
    if not s: return ("", "", "")
    s = s.strip()
    for fmt in ("%Y-%m-%d","%Y-%m","%Y"):
        try:
            dt = datetime.strptime(s, fmt)
            y = dt.year; m = dt.month if "%m" in fmt else 1
            return (dt.strftime("%Y-%m-%d") if "%d" in fmt else f"{y:04d}-{m:02d}-01",
                    str(y), f"{m:02d}")
        except ValueError: pass
    m = re.search(r"(\d{4})", s)
    if m: return (f"{m.group(1)}-01-01", m.group(1), "01")
    return ("","","")

with open(inp, newline='') as f, open(outp, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    cols_out = ["Isolate_Id","Isolate_Name","Subtype","Clade","Host",
                "region","country","division","location","Collection_Date",
                "date","year","month"]
    w = csv.DictWriter(g, delimiter="\t", fieldnames=cols_out)
    w.writeheader()
    for row in r:
        reg, ctry, div, loc = split_loc(row.get("Location",""))
        d, y, m = norm_date(row.get("Collection_Date",""))
        w.writerow({
          "Isolate_Id": row.get("Isolate_Id",""),
          "Isolate_Name": row.get("Isolate_Name",""),
          "Subtype": row.get("Subtype",""),
          "Clade": row.get("Clade",""),
          "Host": row.get("Host",""),
          "region": reg, "country": ctry, "division": div, "location": loc,
          "Collection_Date": row.get("Collection_Date",""),
          "date": d, "year": y, "month": m
        })
print(f"Wrote {outp}")
PY
```
✅ **Checkpoint:** `meta_clean.tsv` exists and has one row per sequence, keyed by `Isolate_Id`.

* * * * *

### 1.3.3. Merge metadata + Nextclade results

We now join the cleaned metadata with Nextclade’s QC + clade calls.

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv

meta_in  = "meta_clean.tsv"
clade_in = "nextclade.tsv"
outp     = "metadata_final.tsv"

clade_map = {}
with open(clade_in, newline='') as f:
    r = csv.DictReader(f, delimiter="\t")
    for row in r:
        # full FASTA header was kept, but Isolate_Id is always the first token before '|'
        acc = row['seqName'].split('|')[0]
        clade_map[acc] = {
          'nextclade_clade': row.get('clade',''),
          'qc.overallStatus': row.get('qc.overallStatus','')
        }

with open(meta_in, newline='') as f, open(outp, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    cols_out = ["strain","Isolate_Id","Isolate_Name","Subtype","Clade","nextclade_clade",
                "Host","region","country","division","location","Collection_Date",
                "date","year","month","qc.overallStatus"]
    w = csv.DictWriter(g, delimiter="\t", fieldnames=cols_out)
    w.writeheader()
    for row in r:
        iso = row["Isolate_Id"]
        nc  = clade_map.get(iso, {'nextclade_clade':'','qc.overallStatus':''})
        w.writerow({
          "strain": iso, "Isolate_Id": iso,
          "Isolate_Name": row["Isolate_Name"], "Subtype": row["Subtype"],
          "Clade": row["Clade"], "nextclade_clade": nc['nextclade_clade'],
          "Host": row["Host"], "region": row["region"], "country": row["country"],
          "division": row["division"], "location": row["location"],
          "Collection_Date": row["Collection_Date"], "date": row["date"],
          "year": row["year"], "month": row["month"],
          "qc.overallStatus": nc["qc.overallStatus"]
        })
print(f"Wrote {outp}")
PY
```

✅ **Checkpoint:** `metadata_final.tsv` is the master metadata file for Augur.

* * * * *

### 1.3.4. Downsample + build tree

We’ll take at most 10 sequences per clade per year, to keep trees manageable.

```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur index --sequences gisaid_ha.fasta --output raw.idx

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur filter \
    --metadata metadata_final.tsv \
    --sequences gisaid_ha.fasta \
    --sequence-index raw.idx \
    --metadata-id-columns strain \
    --group-by nextclade_clade year \
    --sequences-per-group 10 \
    --output-sequences curated.fasta \
    --output-metadata curated.tsv

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur align --sequences curated.fasta --output aligned.fasta --fill-gaps

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur tree --alignment aligned.fasta --output tree_raw.nwk --nthreads 2

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur refine --tree tree_raw.nwk --alignment aligned.fasta \
    --metadata curated.tsv --output-tree tree.nwk --timetree
```

✅ **Checkpoint:** you should have `tree.nwk`, a time-scaled tree.

* * * * *

### 1.3.5. Export to Auspice

```
mkdir -p auspice

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur export v2 \
    --tree tree.nwk \
    --metadata curated.tsv \
    --metadata-fields nextclade_clade Clade date year month region country division location Host \
    --output auspice/gisaid-h3n2-ha.json

docker run -it --rm -v "$PWD":/data -w /data -p 4010:4000 nextstrain/base \
  nextstrain view --host 0.0.0.0 auspice/

# Open http://localhost:4010/gisaid-h3n2-ha
```

✅ **Checkpoint:** tree should open in Auspice, color by `clade`, `year`, `country`, etc.


* * * * *

### 1.3.6. Add your own sequences

If you have **your own sequences + metadata**, you can combine them with GISAID references.

1.  **Prepare your files**:

    -   `myseqs.fasta` → headers like `>MYSEQ_001`

    -   `my_meta.tsv` → same columns as `metadata_final.tsv` (see template below)

    Example:

    
```
strain	Isolate_Id	Isolate_Name	Subtype	Clade	nextclade_clade	Host	region	country	division	location	Collection_Date	date	year	month	qc.overallStatus
MYSEQ_001	MYSEQ_001	A/South_Africa/Seq1/2023	H3N2		Human	Africa	South Africa	Western Cape	Cape Town	2023-05-15	2023-05-15	2023	05	pass
```

2.  **Combine FASTAs**:

```
cat gisaid_ha.fasta myseqs.fasta > combined.fasta
```

3.  **Combine metadata**:

```
(head -n 1 metadata_final.tsv && tail -n +2 metadata_final.tsv && tail -n +2 my_meta.tsv) \
  > combined_meta.tsv
```

4.  **Re-run Augur** with `combined.fasta` + `combined_meta.tsv` instead of the GISAID-only versions.

✅ **Checkpoint:** when you open Auspice, your sequences should now appear in the tree alongside GISAID references.



* * * * *

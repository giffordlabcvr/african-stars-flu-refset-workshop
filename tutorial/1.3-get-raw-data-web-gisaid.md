## 1.3 (Optional): Command-line approach with data downloaded from **GISAID** (web)

-   Use the query builder: A/H3N2, Location = South Africa, Date = 2018--2025.
-   Export: **FASTA + metadata** (CSV/TSV).
-   Consider **[data-use terms](https://gisaid.org/terms-of-use/)**.

Data previously downloaded from GISAID is available **[here](https://github.com/giffordlabcvr/african-stars-flu-refset-workshop/tree/main/data)**, including `FASTA` formatted **[sequence data](https://github.com/giffordlabcvr/african-stars-flu-refset-workshop/blob/main/data/influenza_rsa_2021.fasta)** and `.tsv` formatted **[sequence metadata](https://github.com/giffordlabcvr/african-stars-flu-refset-workshop/blob/main/data/influenza_rsa_2021.tsv)**.

**Inputs you already have from GISAID (downloaded via web):**

-   `gisaid_ha.fasta`  (HA sequences; FASTA headers like `>EPI_ISL_17891586|A/South_Africa/R04355/2023`)

-   `gisaid_meta.tsv`  (tab-delimited; header begins with `Isolate_Id`, `...`, `Location`, `Collection_Date`, `Clade`, etc.)

We'll produce:

-   A cleaned + simplified metadata table compatible with Augur/Auspice.

-   A curated, downsampled FASTA.

-   An Auspice JSON view.

We will use **Docker** for Nextclade/Augur (no local installs), plus a couple of tiny Python snippets run inside Docker.


* * * * *

### 1.3.0. (One-time) Pull required images

```
docker pull nextstrain/nextclade:latest
docker pull nextstrain/base
```

* * * * *


### 1.3.1. Normalise FASTA headers to the **EPI_ISL** id

We downloaded GISAID FASTA headers that look like `>EPI_ISL_XXXXXXX|A/Country/...`. We'll keep only the **left side** of the `|` so FASTA tip names match `Isolate_Id`.

```
awk 'BEGIN{OFS=""}
  /^>/ {
    h=$0; sub(/^>/,"",h);
    split(h,a,"|");
    print ">", a[1]; next
  }
  {print}
' gisaid_ha.fasta > gisaid_ha.acc.fasta
```

Now each header is like `>EPI_ISL_17891586`.
This will become our strain ID as well.


* * * * *

### 1.3.2. Run **Nextclade (Docker)** for QC + clade calls

Download the H3N2-HA dataset once, then run on your (normalized) FASTA:

```
# dataset (cached under ~/.nextclade)
docker run --rm -it -v "$HOME/.nextclade":/root/.nextclade nextstrain/nextclade:latest \
  nextclade dataset get --name nextstrain/flu/h3n2/ha \
  --output-dir /root/.nextclade/datasets/h3n2_ha
```

```
# QC + clades (TSV)
docker run --rm -it \
  -v "$PWD":/data \
  -v "$HOME/.nextclade":/root/.nextclade \
  nextstrain/nextclade:latest \
  nextclade run \
    --input-dataset /root/.nextclade/datasets/h3n2_ha \
    --output-tsv /data/nextclade.tsv \
    /data/gisaid_ha.acc.fasta
```

Checkpoint: nextclade.tsv exists.


* * * * *

### 1.3.3. Clean & prepare **GISAID metadata**

GISAID TSVs often include embedded newlines in quoted fields (e.g., **Publication**). A proper TSV parser handles this; don't try to "fix" with naive line tools.

We'll:

1.  Load the TSV safely,

2.  Split `Location` into `region / country / division / location` (if present),

3.  Make `date/year/month` from `Collection_Date`,

4.  Produce a tidy table keyed by `Isolate_Id`.

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv, re
from datetime import datetime

inp  = "gisaid_meta.tsv"
outp = "meta_clean.tsv"

def split_loc(s):
    parts = [p.strip() for p in (s or "").split('/') if p.strip()]
    parts += [""] * (4 - len(parts))
    return parts[:4]  # region, country, division, location

def norm_date(s):
    if not s: return ("", "", "")
    s = s.strip()
    for fmt in ("%Y-%m-%d","%Y-%m","%Y"):
        try:
            dt = datetime.strptime(s, fmt)
            y = dt.year; m = dt.month if "%m" in fmt else 1
            return (dt.strftime("%Y-%m-%d") if "%d" in fmt else f"{y:04d}-{m:02d}-01",
                    str(y), f"{m:02d}")
        except ValueError:
            pass
    # fallback: just grab any 4-digit year
    m = re.search(r"(\d{4})", s)
    if m:
        y = m.group(1)
        return (f"{y}-01-01", y, "01")
    return ("","","")

with open(inp, newline='') as f, open(outp, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    cols_out = ["Isolate_Id","Isolate_Name","Subtype","Clade","Host",
                "region","country","division","location","Collection_Date",
                "date","year","month"]
    w = csv.DictWriter(g, delimiter="\t", fieldnames=cols_out)
    w.writeheader()
    for row in r:
        reg, ctry, div, loc = split_loc(row.get("Location",""))
        d, y, m = norm_date(row.get("Collection_Date",""))
        w.writerow({
          "Isolate_Id":     row.get("Isolate_Id",""),
          "Isolate_Name":   row.get("Isolate_Name",""),
          "Subtype":        row.get("Subtype",""),
          "Clade":          row.get("Clade",""),
          "Host":           row.get("Host",""),
          "region": reg, "country": ctry, "division": div, "location": loc,
          "Collection_Date": row.get("Collection_Date",""),
          "date": d, "year": y, "month": m
        })
print(f"Wrote {outp}")
PY
```

**Checkpoint:** `meta_clean.tsv` exists and has `Isolate_Id`, `Clade`, `date/year/month`, `region/country/..`


* * * * *

### 1.3.4. Join **Nextclade** with **metadata** and create **Augur/Auspice** metadata

We will:

-   Set **`strain` = `Isolate_Id`** (matches the normalized FASTA headers),

-   Merge in `clade` + a couple QC fields from `nextclade.tsv`.

```
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv

meta_in  = "meta_clean.tsv"       # from G3
clade_in = "nextclade.tsv"        # from G2
outp     = "metadata_final.tsv"

# Map Nextclade by accession (first token of seqName)
clade_map = {}
with open(clade_in, newline='') as f:
    r = csv.DictReader(f, delimiter="\t")
    for row in r:
        acc = row['seqName'].split()[0]
        clade_map[acc] = {
          'nextclade_clade': row.get('clade',''),
          'qc.overallStatus': row.get('qc.overallStatus',''),
          'totalMissing': row.get('totalMissing','')
        }

with open(meta_in, newline='') as f, open(outp, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    cols_out = (["strain","Isolate_Id","Isolate_Name","Subtype","Clade","nextclade_clade",
                 "Host","region","country","division","location",
                 "Collection_Date","date","year","month",
                 "qc.overallStatus","totalMissing"])
    w = csv.DictWriter(g, delimiter="\t", fieldnames=cols_out)
    w.writeheader()
    for row in r:
        iso = row["Isolate_Id"]
        nc  = clade_map.get(iso, {'nextclade_clade':'','qc.overallStatus':'','totalMissing':''})
        w.writerow({
          "strain": iso, "Isolate_Id": iso,
          "Isolate_Name": row["Isolate_Name"], "Subtype": row["Subtype"],
          "Clade": row["Clade"], "nextclade_clade": nc['nextclade_clade'],
          "Host": row["Host"], "region": row["region"], "country": row["country"],
          "division": row["division"], "location": row["location"],
          "Collection_Date": row["Collection_Date"], "date": row["date"],
          "year": row["year"], "month": row["month"],
          "qc.overallStatus": nc["qc.overallStatus"],
          "totalMissing": nc["totalMissing"]
        })
print(f"Wrote {outp}")
PY
```

**Checkpoint:** `metadata_final.tsv` has **`strain`** as the first column and includes `date/year/month`, `Clade` and `nextclade_clade`.


* * * * *

### 1.3.5. Optional filter by country/host/clade (pre-curation)

If you want to subset to (say) **South Africa, Human, H3N2** before downsampling, you can filter `metadata_final.tsv` with a tiny script and then subset FASTA accordingly. (Skip if your download is already the set you want.)

```
# Example: keep only South Africa + Human + subtype contains H3N2
docker run --rm -i -v "$PWD":/data -w /data nextstrain/base \
  python3 - <<'PY'
import csv
keep_meta = "metadata_final.tsv"
out_meta  = "metadata_sa.tsv"
with open(keep_meta, newline='') as f, open(out_meta, "w", newline='') as g:
    r = csv.DictReader(f, delimiter="\t")
    w = csv.DictWriter(g, delimiter="\t", fieldnames=r.fieldnames)
    w.writeheader()
    for row in r:
        if row["country"] == "South Africa" and "H3N2" in row.get("Subtype",""):
            w.writerow(row)
print("Wrote", out_meta)
PY

# Subset FASTA to those strains
awk 'NR==FNR{keep[$1]=1; next} /^>/{id=substr($0,2); p=(id in keep)} p' \
  <(tail -n +2 metadata_sa.tsv | cut -f1) gisaid_ha.acc.fasta > gisaid_ha.sa.acc.fasta
```

Then use `metadata_sa.tsv` + `gisaid_ha.sa.acc.fasta` in the next steps.


* * * * *

### 1.3.6. Downsample with **Augur**, align, build tree

Here we demonstrate with the **entire** prepared set; switch to your filtered files if you did G5.

```
# Index
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur index \
    --sequences gisaid_ha.acc.fasta \
    --output raw.idx
```

```
# Downsample: group by clade & year (tune as needed)
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur filter \
    --metadata metadata_final.tsv \
    --sequences gisaid_ha.acc.fasta \
    --sequence-index raw.idx \
    --metadata-id-columns strain \
    --group-by nextclade_clade year \
    --sequences-per-group 10 \
    --output-sequences curated.fasta \
    --output-metadata curated.tsv
```

**Alignment** --- either:

-   Use a **master reference** (e.g., CY033009.1 HA):

```
efetch -db nucleotide -id CY033009.1 -format fasta > reference_h3n2.fasta

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur align \
    --sequences curated.fasta \
    --reference-sequence reference_h3n2.fasta \
    --output aligned.fasta \
    --fill-gaps
```

**OR** have Nextclade produce an aligned FASTA and skip `augur align`:


```
docker run --rm -it -v "$PWD":/data -v "$HOME/.nextclade":/root/.nextclade nextstrain/nextclade:latest \
  nextclade run --input-dataset /root/.nextclade/datasets/h3n2_ha \
    --output-fasta /data/nextclade_aligned.fasta \
    /data/curated.fasta
cp nextclade_aligned.fasta aligned.fasta

```

Tree & refine


```
docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur tree \
    --alignment aligned.fasta \
    --output tree_raw.nwk \
    --nthreads 2

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur refine \
    --tree tree_raw.nwk \
    --alignment aligned.fasta \
    --metadata curated.tsv \
    --output-tree tree.nwk \
    --timetree

```

### 1.3.7. Export to Auspice & view (explicit color fields + host binding)

```
mkdir -p auspice

docker run -it --rm -v "$PWD":/data -w /data nextstrain/base \
  augur export v2 \
    --tree tree.nwk \
    --metadata curated.tsv \
    --metadata-fields nextclade_clade Clade date year month region country division location Host \
    --output auspice/gisaid-h3n2-ha.json
```

Serve on a port that’s free and bind to all interfaces:

```
docker run -it --rm \
  -v "$PWD":/data -w /data \
  -p 4010:4000 \
  nextstrain/base \
  nextstrain view --host 0.0.0.0 auspice/

# open http://localhost:4010/gisaid-h3n2-ha
```

In Auspice, try **Color by**: `nextclade_clade`, `year`, `month`, `country`, etc.

* * * * *

Notes / Gotchas
---------------

-   **No color options?**\
    Ensure **FASTA headers** (we normalized to `EPI_ISL_*`) match the **`strain`** column in metadata (`metadata_final.tsv` → `curated.tsv`).\
    If you filtered to a subset, re-subset FASTA *and* metadata together.

-   **Dates aren't grouping?**\
    We derived `date/year/month` from `Collection_Date`. If many rows lack dates, you'll have fewer groups.

-   **Want strict reproducibility for subsampling?**\
    The `nextstrain/base` image you're using may not support `--seed`. Switch to `nextstrain/cli:latest` for newer Augur with `--seed`.

-   **Remember GISAID terms.**\
    Keep outputs within permitted sharing boundaries (e.g., don't publish per-sample metadata outside allowed contexts).

    
* * * * *
